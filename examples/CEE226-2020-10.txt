 Welcome to this lecture on uncertainty in LCA. You know, uncertainty in LCA is one of the most important topics that we have to consider as we start to move towards that interpretation phase of the ISO 14-040 series standards. And so the reason that uncertainty is so critical is because, well, often we're modeling things that are far off in the future or systems which don't yet exist. That's particularly the case, of course, with consequential LCA. And so therefore we have to, rather than sort of shy away from the consideration of uncertainty, actually take it head on and start to think about how uncertain or robust our decisions are that are based on a life cycle assessment. So in this case, we're going to be talking about concepts of why we want to deal with the uncertainty that's in our modeling, specifically what's the value of dealing with that. What are the various sources of uncertainty in life cycle assessment, the methods to deal with it and report it, and then finally how you can start to use this for some of your models in your own projects. So the purpose of uncertainty analysis is to determine the robustness of the decision that we're going to make using information from our life cycle assessment. And it allows us to understand both the precision of the results, how accurate do we think this is, and the robustness of the decision, whether or not we would continue to make that same decision every time, if indeed some of the results of the analysis changed due to uncertain conditions. And as I mentioned, this really starts to move us over into the interpretation phase of the LCA ISO 14040 framework, having done goal and scope definition, inventory analysis, and impact assessment, all of which have a component related to uncertainty. All right, so why do we explicitly deal with uncertainty? Well, you know, LCA is an increasingly popular tool for measuring environmental performance and sustainability of a variety of products and processes. And as we've learned, it's an analytical technique for assessing potential environmental, social, and economic burdens and impacts of a given product or process. And it encompasses all the stages of the life cycle from raw material production through to end of life management. But each stage of the life cycle, as we'll see, has significant associated uncertainties. And the typical practice in many cases is to neglect these uncertainties and simply say, look, we're going to have a deterministic single value, but this may lead to flawed decision making. And so, therefore, we are going to, once again, explicitly deal with uncertainty. Now, as far as dealing with the value of uncertainty around decision support, you know, there are apparent differences in impacts that may be misleading if uncertainty in the impacts is large enough to overwhelm any relative differences between the alternatives. Meaning, if we have two specific products or processes, like what you're looking at in your term project, and all of a sudden, you know, the uncertainty associated with the differences between them is much larger than those actual differences, then you really can't say which one of these two alternatives is better. You know, other fields, such as pharmaceutical testing and all kinds of fields, consider this type of issue explicitly. And what we're really looking for here is, and the term that we want to look here is significance. Also, by considering uncertainty explicitly, we can also start to move forward with things like hypothesis testing and other robust statistical tools for trying to deal with uncertainty. Furthermore, part of the value of dealing with uncertainty is transparency. If the model inputs are uncertain and the uncertainty is hidden or ignored by the analyst, then this lowers the credibility of the LCA. Coming out and saying, you know, look, this is exactly what I think the number is. And it just opens up to lots of questions. And so by explicitly reporting the uncertainty associated with the study is incredibly important. You know, if the uncertainty is too large for the final audience, then this is not a bad thing. Rather, it points towards the need to collect and generate better underlying information that can help reduce that level of uncertainty associated with the study. Furthermore, it promotes quality competition. So by transparently displaying uncertainties, there are going to be increased motivation to improve quality data, for example, in standardized databases. We've seen industry after industry start to provide better and better data once they see that other industries, maybe competing industries, are providing much more certain data on what some LCIs are going to look like. And so therefore, that sort of provides a competition among competing sort of raw materials. And the concrete and the steel industry have long been trying to provide better and better data because they want to try to make their LCAs more certain and more credible. And then it also helps you plan information gathering efforts. You know, if the sources are available to, if the resources, money, are available to refine the elements of an LCA analysis, it's helpful to understand what uncertainties have the greatest impact on results. If you're going to spend more time and more money focusing on collecting more data or trying to get to a better answer, you want to know where most of that uncertainty is actually coming from in your model. As we've learned throughout a number of lectures, 80% of the impacts come from 20% of the inputs. Well, it often works the same with uncertainty. Much of the uncertainty associated with our studies is located in some pretty specific smaller parts of those studies. You know, for example, you may think, well, gosh, I simply don't know how many miles or kilometers of transport are associated with the distribution phase. Well, if the distribution phase is truly a very small part of the overall impact, then you could be off by two or three times the distance in your transportation modeling, and it may not affect the overall robustness of the decision. And so trying to, you know, plan further information gathering efforts, not wasting time and resources on trying to get exact numbers on values that truly aren't going to reduce the overall uncertainty of your study or increase the robustness of your decision. So that's the overall value of just explicitly dealing with uncertainty in a highly transparent and highly rigorous way. So what kinds of uncertainty do we see in various types of LCA? And we'll go into each of these in a little more depth. First of all, we're going to talk about modeling uncertainty. You know, simplified models may not capture the complex cause and effect mechanism, cause and effect mechanisms that we see in a lot of complex supply chains. And if we're using sort of regress data or large data sets, it may have the wrong functional form. We can also get uncertainty in the database. The available data in an LCA database may not exactly represent the quantity being studied. And I think many of you are starting to see that even though there are lots of processes in our databases, not every single process is in there. Far from it. And so therefore, the available data may not exactly represent what you want. So therefore, you're using proxies or other substitutes for that data. That is absolutely a source of uncertainty. And how do we quantify it? Well, often we'll try a couple different proxies and see how much that changes the results of our study. There's uncertainty in preferences. Preferences in goal and scope definitions, in analyst preferences. These play a big role in LCA and may not be clearly defined. And so being a little bit uncertain on what's included, what's not included, how that functional unit is defined can introduce uncertainty. And that's simply an analyst preference. Uncertainty in a future physical system versus a design system. You know, the physical system may not match the design system once you actually get it into place. This is due to the substitution of materials, late stage design changes, or changes in the use phase of the system. You know, this is one of our biggest things with a consequential LCA. And, of course, the reason is that's actually a design system that is not yet in place. And so, therefore, there's a high likelihood that what actually gets put into place or into practice is not exactly modeled. And then, finally, statistical error and measurement. You know, impact estimates from limited data sources will have statistical variability. And that's simply a part of any data collection. And so, therefore, we want to also embrace that in our handling of uncertainty. So let's look a little more deeply at each one of these types. So in terms of modeling uncertainty, we can look at sort of this model uncertainty associated with the extrusion process, right? And so we might have uncertainty in the final production alloy of what's being produced. Here we're looking at a number of different metals. We may not be certain exactly what's going to be used. And in our model, we might not know exactly what temperatures and what extrusion constant we're going to end up actually performing some type of extrusion associated with, for instance, copper. And so, therefore, we're not actually sure how much heat that we're going to have to get into that process or at what pressure we're going to have to actually carry out this model. So what might we do? Well, we might end up saying it could be anywhere in this range equally. Or we might say, well, on average, we think it's there and put some distribution associated to it. And so, therefore, you know, that's going to generate uncertainty. But what we've done by saying that is that we know that this copper will be extruded at a temperature somewhere between roughly 900 and 1600 degrees Fahrenheit. Wonderful. Wonderful. What we now have to do is then either get more information from the actual producer to hone in on that or simply conduct, as we'll see, sensitivity analysis and more uncertainty analysis to understand what that range will have on our overall model uncertainty. All right. It's own has its own lifecycle inventory associated with it because they're generating electricity from different sources. Right. Out west, we're going to get a lot more hydro. In the middle of the country, we're going to get a lot more coal. And so, therefore, they have different impacts. Now, if you're uncertain where production is going to take place, if it's happening in, say, Iowa versus Nebraska versus Kansas, you know, this was going to lead to a difference in inventory. And so, if you're uncertain where it's going to happen, that can be a source of uncertainty. And so, as we had seen before, right, comparing between these two different regions for, say, fossil fuel, carbon dioxide emissions, right, will absolutely create a difference, a significant difference. Right. So, what might we do? Well, we might use a U.S. average, right, and just simply say, okay, we're uncertain where it's going to be. And so, an average is acceptable. Well, if that's, in fact, what we want to do, that's okay. But then we can also take a high number and a low number and say, look, we're uncertain where it's going to happen. It could be as high as this. It could be as low as that. And so, that's one way for us to quantify, once again, that range of values over which we can consider the uncertainty. All right. So, next up is goal and scope uncertainty. You know, so, limited scope studies may offer greater precision with lower accuracy. And the idea being that, in this case, we're looking at the impacts associated with furniture production. And so, of course, it starts with raw material extraction, transportation to a supplier, transportation to, in this case, a wood furniture producer, transportation to the user, and transportation to end of life. Now, if we draw our system boundary, simply taking raw material acquisition all the way to the end of manufacturing. So, this is going to be what we call cradle to gate. It's the gate at which it leaves the factory before going finally to the customer. We can have much more precision in this overall LCA. Why? Because there's a lot of uncertainty about how users will ultimately use a product. And then, at end of life, if we know when end of life actually happens, where it will go. And whether or not there's any kind of recycling. So, therefore, by drawing the boundary in this case, we can improve precision. But it might not be as accurate because, as a user, as someone who wants to buy this wood furniture, I kind of want to know the entire lifecycle impact, which my use and the way it's treated at end of life is an important part. And so, therefore, we have given greater precision, but ultimately lowered the accuracy potentially. The reason we can get that greater precision is because we're probably going to know, as the company, where some of these materials are being sourced and what the supply chain looks like, leading to our actual operations. There we have some form of managerial control. But once again, it might not be as accurate. A broader scope study, like what we're seeing here, will probably offer higher uncertainty. Why? Because we've got a lot of uncertainty here and a lot of uncertainty here at end of life, far out into the future. But it may offer greater accuracy because it encompasses the entire system. And transparency, because it includes all the components of the entire lifecycle. Right? Not just simply those that are cradle to gate. All right. So next up, I want to talk a little bit about predicted versus actual modeling uncertainty. Once again, what we're really trying to understand here is, you know, in consequential LCAs, for which we don't actually know what's going to happen yet, how accurate are our data-driven sort of quantitative sustainability analyses at very early stages when we don't have much information? You know, as changes and decisions are made during a design process, how much uncertainty is actually introduced? And so this is coming from a study that we had done, once again, with Steelcase a number of years ago. And the way that the design phases work at a company like Steelcase doing furniture design is they start with a concept. And at that concept stage, they're really just trying to determine what does the customer want and sort of what is the general concept of the design. So then they'll move into actual design, where they'll start to do some, you know, here's the size of a chair, here's the size of the table, some general massing, maybe some material selection, things of that nature. Finally, then, they're going to go into actual engineering, where you'll start to come out with specific engineer drawings, where you're computing stresses, you're computing strains, you're actually starting to put some specificity onto the design. Then managing the supply chain process, the manufacturing, going into tooling. Now, at this point, you're actually down to where we have all of the information that we could start to use to really push forward this analysis. And then finally, all the way to reporting, where you've built the entire supply chain, you're actually selling these products now. And so no longer is it a consequential LCA, it's an attribution LCA. Everything's been decided, this thing's in full production, and we're actually moving forward with it. Now, interestingly enough, you really want to start to make decisions using LCA early on. You know, performing an LCA here, after everything's already been set in stone, so to speak, isn't really all that useful in many regards. Because, well, I mean, once you've decided on supply chain and signed contracts and built tooling and mobilized a large manufacturing organization, and then you find out that your footprint for your new product is too high to be considered for, say, commercial lead buildings where they need to get a certain footprint, or that it's just simply not that attractive in comparison to, say, a competitor's product in terms of their environmental impact. You want to have the LCA guide your design from the earliest stages so that you can hit targets later on. And so we want to understand what are the differences between predicted impacts, which is what we're going to have at this stage, to the actual impacts, which is what we're going to be measuring down here. So what we did in this project is we took a product that was already under production. We had done a final report, and that was what we considered to be our ground truth. Everything was set. We had done a full ISO certified LCA on a specific piece of office furniture. Then we worked with the designers to recreate the design process so that we could understand what would have been known at every single stage of the design process. And then recreating that accumulation of knowledge or that accumulation of decisions throughout the design process allowed us to recreate the accuracy of the overall process, which ultimately gave us insight into how much we could try to push data back in further up into the design process. So is that kind of data actually useful? So in this case, the way that we set it up is we said, okay, at this stage, everything shaded in gray, that's going to come from sort of standard databases around the fabrication of specific materials. If there are custom parts and things of this nature, that was information we got directly from suppliers. This came from the plant itself. And then we could use general scenario data on customers, on how they typically use the furniture, and then dispose of the future. So there's a mix between standard data here and here, custom data provided by suppliers, and then semi-standard data if it's coming from sort of custom part fabrications and some other things. Data was being pulled from all over the place for basic materials coming from SEMA Pro for supplier processes. Specific supplier-specific profiles were provided. For the steel case processes, we had actual access to local and regional process profiles from energy profiles, from their plants and things like this. For transportation, we used distances to typical distribution points and markets and averaged those. And then at the end of life, we kind of knew what the standard profile was for a steel case product, whether it was just return for clothes with recycling, for the metal, or if it was just simply landfilled in the case of most of the wood products. What you end up finding is that you accrue impacts, typically throughout the design process. And the reason is, as you get more and more information, you start to include more and more processes and more and more things into your LCA. And so at early stage, we were looking at a weighted impact score here. And then as we get more and more additional details to finally the full answer, we get to this point. And so we get better and better information as we move further up the design process. Now, interestingly enough, we wanted to try to understand, well, how accurate do we have to be at every single stage of the design process? And one of the ways for us to try to understand that using steel cases sort of guidelines is that in addition to the, you know, the footprinting, the environmental footprinting for their green products, they also have targets at every single stage of design for what the cost of goods sold should be. And the reason that's important is normally when they introduce some new furniture product, they have a specific price point that they're trying to go to market at for the customer. They already have some idea out there, the competitive space. They know how much it's going to, how much a customer is going to want to pay for this. And so they have a cost of goods sold target. So they want to know how much is it going to cost them, how much is it going to cost them to produce this piece of furniture so that when profit is added, they have a price that gives them a decent competitive advantage in the market. Well, if this is your target, you need to have estimates. Early on. Of what you think it's going to cost you to produce this. And those estimates are, of course, based on typical supplier data, typical material prices, commodity prices, things like this. And then, of course, as you get further on and make decisions, those estimates will get more and more refined until finally, once you're selling it, you know exactly how much it costs you to produce this product. And the measures, excuse me, the ways in which they estimate those, the heuristics that they use to estimate cost of goods sold have been refined over, you know, decades of work going on in the design office. They're now actually quite confident that they can hit a certain set of cost of goods sold at the end. So their guidelines for trying to estimate financial or the cost of goods sold are such a, at the concept stage, there's no guideline. At the design stage, they want to be within 30% of the cost of goods sold target. And they have found that using their estimation procedures, they can actually get there. At the engineering stage, they want to be, so this is design, engineering. And then finally, once they have the supply chain set up, the tooling, everything else designed, they want to be within no more than 10% of final cost of goods sold. And that's what they've seen. They've been very successful at putting forward by using their current financial accounting systems. Well, when we did our LCA study, this one specifically to say, okay, how close can we get early on at the design stage, at the engineering stage, at the processing stage, and then finally comparing that to the full report, we found that at the design stage, we were within about 32% of the guidelines of what the final product LCA looked at. And then that came down to about 7% once we got to engineering, and 5% once we got down to final processing. Now, the reason it drops so much here, as we've learned, so much of the impact comes from the production of the raw materials themselves. So once you have the masses of those materials set, everything else that goes on in the specific design of the processing and tooling, while those may have larger financial deviations, they're not going to have very large environmental impact deviations. And so we actually found that once you get to about this stage of the design process, we can get quite accurate at trying to measure what the final impacts are going to be. And so uncertainty drops very quickly once you get to sort of this phase of the process. Once you know the masses of a lot of these materials, that clears up a lot when it comes to transportation, because that's half of the transportation calculation, but also a lot when it comes to the very energy and input intensive sort of raw material acquisition and material processing phases of the life cycle. So, you know, as we're thinking about trying to control uncertainty and trying to minimize uncertainty, it's really, you know, in these stages that you see the most uncertainty, but then it drops quite precipitously once you get to actual engineering and identification of the supply chain. All right. So that's the way accuracy changes throughout the design process. Now, there's also uncertainty in the statistical error and measurement in the databases. And we actually looked at this previously. This is one of the environmental emissions from one of our energy sources. And we looked at those data quality indicators over here, which provide us some kind of a GPA associated with this type of data. We can also have sources of error in database measurement. You know, there is variability and stochastic errors in the figures which describe the inputs and outputs. This is due to measurement uncertainty, process-specific variations, temporal variations. There's also a question of the appropriateness of the input or output flows. You know, sometimes it doesn't match perfectly with the input or output observed in reality. And this may be due to temporal and or spatial approximations. So, for example, the electricity consumption of a process that takes place in Nigeria is approximated with a data set on the electricity supply mix of the European network. Well, yeah, those two are not going to be the same. And so we may have a number of differences arising from the use of those proxies. And you're going to have to understand a little bit of that in your own LCA studies. One of the best ways for us to try to measure it or at least get a handle on it is to try a number of different proxies, substitute them in, and see what happens to your modeling results. Once again, there's modeling uncertainty. And the model used to describe the unit process may be inappropriate. Using, for instance, a linear instead of nonlinear model. For a lot of processes, there are economies of scale. And if you just simply say, well, you know, let's just take this one unit process and multiply it by 1,000 to get 1,000 of the output. Well, it normally doesn't work that way. And so there we'll often use a power loss scaling factor, which we can use to estimate the sizes of equipment or the costs of different processes. Or other types of nonlinear models. Now, often we can also neglect important flows. Sometimes not all the relevant information is available to completely discover the process. And so, therefore, these are just missing from the inventory. That creates uncertainty. And so we want to understand all of that uncertainty. Now, the uncertainty in this case is captured within database measurement in the EcoInvent dataset. And they've actually tried to do a lot of work around quantifying the uncertainty and characterizing the uncertainty associated with the LCI databases. So within EcoInvent and many databases, there are four statistical distributions that are often used to model uncertainty in the database. Log normal, normal, triangular, and uniform. You know, due to the multiplicative nature of many processes and the non-negative nature of most emissions datasets, the log normal distribution is the most commonly used distribution. Because when you multiply a couple of normal distributions together, you get a log normal distribution. And so if you're going to be looking at what it takes to produce a specific component, often you'll end up with a mass times some type of process amount. You're multiplying those two together, and you're going to get a log normal distribution for both the material production and the processing that's associated with it. But we'll talk about each one of these in detail. Don't worry. You're not going to need to know all of the mathematics associated with each one of these. You'll just need to know a little bit about how they are used. So a log normal distribution is, like I said, used often because, number one, it's non-negative. You can't have negative values coming out of a log normal distribution. And that's important because you can't, in most cases, have negative emissions. For most products, if we're uncertain how much emissions there are coming from a certain process, we know it's not going to be negative. And so, therefore, a log normal distribution is useful in this case. Furthermore, like I said, if there's two components that are being multiplied, a mass times a distance or a mass times a process amount, if both of those are log normal, excuse me, if both of those are normal, then we're going to get a log normal distribution for the multiplicative nature of those. Normal distributions are also commonly used. You're going to be probably most familiar with a normal distribution. And so this is our standard Gaussian distribution that just provides, you know, an arithmetic mean and then a couple of standard deviations away, providing us, you know, minimum and maximum values. Triangular distributions are also very commonly used. And the reason for this is while a specific process may not have a lot of statistical data to be able to create an entire distribution, distribution, right, for log normal or not, right? Often if you go and you ask folks on the factory floor or you ask folks that are estimating the process, you know, look, how much energy have you seen as a minimum to be, you know, operating this machine, this production line? What's the most you've ever seen? And what is it most often? Well, based on that, we can set up a triangular distribution. And that provides us at least some distribution associated. And so really what we're looking for is a min, a max, and a mode, right, a most likely value. Sometimes we'll also use uniform distributions in order to try to set these as well. And so that can be useful if, in fact, we only know the minimum value and the maximum value. And then just simply say anything in between that is equally likely. All right, now, so for most of these, we're going to need to know, at least for some of them, we're going to need to know a standard deviation. And so how do we, in fact, get to that standard deviation? Well, very rarely do we know what the full distribution is going to be. We are probably going to know a minimum. We might know a maximum. But how do we get to a standard deviation? Well, standard deviation really quantifies that level of uncertainty. How much spread do we have in our overall distribution? And what we end up using in this case is the square of the geometric standard deviation on a 95% interval. So what does that look like? Well, this is the way that we are going to try to set up that overall standard deviation. And we're going to base it on a number of different U factors, seven of them specifically. And each one of these U factors is tied to a different type of uncertainty that we have with that product or process. U1 is uncertainty on reliability. U2, sample size. U3, technological correlation. U4, geographic correlation. 5 is temporal correlation. 6 is completeness. And finally, this last one, the seventh one, UB, is a basic uncertainty factor. So what we will then do is go out and discuss with the folks in the field, doing a little bit of primary data analysis, qualitatively how certain are they on all of those different factors. And this is the way in which it works. So, for instance, for reliability, is the data verified based on measurements? Is it verified data partly based on assumptions or non-verified data based on measurements? Non-verified data partly based on qualified estimates? Qualified estimates? Or just a non-qualified estimate? And so, and some remarks. So, for each one of these, you will select a value of 1 to 5. In some cases, there is no value, right? That relates to that U1, U2, U3, U4, U5, and U6. And you can see here, you know, a value of 1 for U6 means more than 100 measurements with continuous measurement on a balance of the overall products. More than 20 measurements, more than 10 measurements, you know, less than 3 measurements. I have no idea how many, or more than 3 measurements. I have no idea how many measurements. So you can use that to define your factors U1 through U6. Now, based on expert judgment, these then end up being your U values, right? So if you have a, for liability, a value of 1, that U factor that goes into that large equation to get the 95% standard geometric, standard deviation, then that value for U is 1, 1.05 if you have a 2, and so on. And so this allows us to now sort of construct that entire calculation. Okay. And then finally, sort of the basic uncertainty factor that then deals with whether or not you're dealing with some kind of production, excuse me, combustion emission, some process emission, or some, in this case, agricultural emission, associated with the kind of input-output group that you're dealing with, right? Are they demand of a certain amount of material or resources? Are they pollutants emitted to water, pollutants emitted to soil, or pollutants emitted to air? And so this is the input side, and here we have the output side to measure that overall B for some type of emission that's coming out. So ultimately, what that gets us to is the distribution, right? Because we're going to know what the expected value is, right? We're going to know what that mean is, and then that standard deviation allows us to create qualitatively the rest of the distribution. All right. All right. So once we have the uncertainty identified and the degree of uncertainty is characterized, what do we then do? Because we have uncertainty coming from our modeling, our database, our preferences, uncertainty in the future physical system versus what we have designed, and then, as we just saw, statistical error and measurement. So there are a number of different ways in which we can think about propagating uncertainty within the LCA. We can use approximate first-order second-moment analytical methods for those that have taken probability and statistics. You might be familiar with these form or swarm reliability methods. Also, sensitivity analysis, and normally we'll be looking at that in the context of tornado diagrams, and then finally, Monte Carlo simulation as well. So tornado diagrams are a great way to show the results of a sensitivity analysis. So basically, what ends up happening is you run your sensitivity analysis, right? So you run your initial model, and that ends up being the center of your, quote, tornado. And then you start taking different assumptions and changing them one at a time, sort of a univariate approach. And say, okay, if the growth rate is as high as, you know, is as high as it could possibly be or as low as it could possibly be, how does that change my overall results? How sensitive is my overall result to the growth rate that's assumed in this study? Now I'm going to change to the area that's planted. How sensitive is my overall final result to the area planted that was assumed in the study? And then, as you can see, sort of why it's called the tornado diagram, right? It leads to us understanding what are the most sensitive assumptions, which what you'll then want to do is say, all right, if I have to spend a little more time to reduce the uncertainty associated with my model, I would want to really focus in on these things up here, because they tend to be the most, the largest contributors to uncertainty. I would want to get more information on exactly what is the growth rate, so that if I get a better determination of that growth rate, I can pull in that uncertainty boundary. So in order to construct a tornado diagram, you've got to select a dependent variable and select some category of interest, right? So that can be CO2 equivalents or anything like that, right? And in this case, you can see their dependent variable is the carbon sequester. That's what they're ultimately going to quantify their uncertainty by. Then define your independent variables. It could be product durability, transportation distances, product masses, whatever you'd like to start to try to understand how that assumption plays into the uncertainty of your overall model. You're going to define a range for that independent variable. You want a maximum value, an expected value, and a minimum value for each one of those independent variables. The expected value is sort of your initial first tick, right? And so that's going to, with all of your expected values in place, that's going to be your center line for your tornado. And then you're going to start to, one at a time, you know, change everything to minimums and maximums. Sometimes if you don't know a minimum or a maximum, you can say, look, we're going to vary this up 10% and down 10% to see how much of an effect that that type of level of variation or up 50% and down 50%. How likely is it, do you think you're going to know that number within a given range? And then see what that does. If you're taking one specific independent variable, like transportation distance, and you, you know, increase it by 100% and then decrease it by half, and it ends up that in your final model, it changes the overall CO2 equivalence for the entire system by a half a percent, then look, that's not a very, your model's not very sensitive to something like transportation distance. And it's highly unlikely you're going to be, you know, a factor of two off of the actual transportation distance that's being undertaken. So once again, then you run the model individually varying independent variables. And so you'll do that in software like SEMA Pro in which you'll say, okay, I've got my, I've got my standard model. That's my center line. I'm going to vary these things one at a time and continue to run the model over and over and over. And when I say over and over, we're talking about picking at most, you know, five or 10 different assumptions that you think might be important, and then running the model again with, with, with each one of those mins and maxes for, for those various, for those various inputs. So you're talking about maybe running the model somewhere between, you know, 15 to 30 times, not a, not a tremendous amount. Once the model is built, all you're now doing is changing specific numbers within that model. And then graphing the results vertically, ranking from the most to the least sensitive independent variables. Once again, that's where that tornado structure comes from. So you can easily see which parts are the most sensitive leading into your overall impact. This is a comparison of transportation effects on SOX emissions for a project, a project that was done in a, in a previous class, a number of years ago for the production of wind turbine blades. And here they wanted to look at the transportation effects on, in this case, the resin and, and the balsa wood that was going into the, into the wind turbine blades. And what they found here is that the, the, the transportation distance on the balsa wood frame was not nearly as important as trying to make sure that you understand what the transportation distances were on the polymer resins that were being used. In this case, it had, the polymer resin was a new thing coming from a materials producer called ever. And that was the trade name for the resin. And so we can see that, ah, how you would interpret something like this is, all right, I now know that, yeah, if I'm going to spend a little more time trying to reduce uncertainty in my model, I don't want to focus on where the balsa wood's coming from or how it's getting there. What I do want to focus on is, ah, if, if, if, if, if, ah, Sox emissions are, are what I'm interested in trying to really understand the certainty around. Yeah. Really honing in on that transportation distance and mass for the resin, because that's really where a lot of our uncertainty in this model came from. So this is just two variables that they changed. Um, but you can already start to see that sort of tornado diagram, um, effect. And so to interpret a tornado diagram, you want to, it helps us to identify the most important modeling variables or the surrogates that have been selected for those variables. Decide where to put more resources in that model refinement. Like I said, in the case of that resin, um, they wanted to know a lot more about the transportation, the resin and not the balsa wood. And that's a really good, uh, outcome from your uncertainty analysis. Uh, it helps you to illustrate the extent of the variability in the model. Um, but it does fail to capture the combined or correlated variability. It's only a partial derivative approach, meaning you're only changing one thing at a time. Now you can start, um, you can start to, to, to, to look at multiple, uh, and change multiple things at the same time. But now what you're doing is you're coming up with a lot more scenarios and, and, and, and there you can get, you know, dozens and hundreds of permutations of things in your model. You could all be changing at the same time to try to understand the uncertainty. If you're really going to get to that level, we no longer are looking at sensitivity analysis and tornado diagrams, but rather we need some more robust quantitative tools to look at all of the different things that can vary at the same time. And for that, uh, we, we use, uh, we'll often use something like Monte Carlo methods. So Monte Carlo methods are a set of computational methods that characterize the variability of systems based on repeated sampling of random variables selected from a defined probabilistic distribution. Like those that we looked at before log normal, triangular uniform and Gaussian normal. Now, this term was first used by physicists working on the Manhattan project at the Los Alamos national laboratory in the 1940s, when they started to look at, uh, uncertainties and probabilistic analysis associated with the Manhattan project. And so because of its analogy to gambling, they, they called the Monte Carlo method. Monte Carlo method. Monte Carlo method. Monte Carlo method. All right. So how do we apply a Monte Carlo method? Well, so let's look at that cup example that we talked about earlier in a lecture, uh, when we were hosting a party. Now, as part of that, a part of that work relied on us computing the impact associated with the manufacturing of each cup. And so in order to jog your memory, we said that, oh yeah, we had two types of cups. One was a polypropylene cup that was reusable and a polystyrene cup, uh, that was, uh, that was disposable. They had a specific mass. And these were the masses that we had. Each one of those had a specific energy in mega joules per kilogram associated with the material production. And a manufacturing energy associated with each one of those manufacturing processes in terms of mega joules per kilogram. To compute the energy that we need to make the cup, we applied this formula. We said the energy in total was equal to the mass times the energy to make the material plus the energy to do the manufacturing. You know, but the values provided for the mass, the energy to make the material and the energy to actually do the manufacturing are actually random variables. I don't know that every single cup weighs exactly the same amount. Uh, there's a little bit of variation. I don't know that the energy to produce that material that, uh, that, that came out of that batch was exactly in this case, 80.03 mega joules per kilogram. Uh, and I don't know that the plant consumes only 14 mega joules per kilogram. There is variation. So therefore the total energy to manufacture it should also be a random variable. Because if we have variation here, here and here, there absolutely has to be variation here. Now, do we just add up that variation? Actually, no, we don't. We can't do that because of that multiplication. So what we'll do is we'll use a Monte Carlo simulation. So we need to know how each of the independent variables are distributed or probabilistically characterized. In this case, we're going to say that the mass is uniformly distributed with a minimum of 121.42 and a maximum of 122.92 for the polypropylene cup. For the polystyrene, it's also uniformly distributed with a minimum of 2.8 and a maximum of 3.56. For the manufacturing, it's log normally distributed with a mean of 80.03, standard deviation of 4. The polystyrene, mean and standard deviation. For the manufacturing, here we're going to say it's normally distributed, mean, standard deviation. For the polystyrene, normally distributed, mean, standard deviation. Sometimes we'll end up seeing a coefficient of variation being specified. And that's just the standard deviation divided by the mean. And it's often, you know, done as a percent. So for instance, the coefficient of variation for the energy, for the materials, yes, in this case, is 5%. So that means in order to produce the polypropylene cup, we're going to be pulling randomly from a uniform distribution on the mass, a log normal distribution on the material, and a normal distribution for the manufacturing energy. And so that's what each one of these distributions ultimately looks like. And so what we're going to do is we're going to pull a random number from this distribution to represent the mass. We're going to pull a random number from this distribution to represent the energy and materials. And we're going to pull a random number from this Gaussian distribution to represent the E in the manufacturing. Once we multiply all those, once we add these two random numbers, and then multiply it by that random mass, we're going to get some energy total that's a random variable. Then we're going to do it again and again and again and again and again. And that's us making many different cups again and again. And so we will continue to do this calculation repeatedly, pulling randomly each time, and seeing how does that affect the overall distribution of our total energy going into a cup. And that will allow us to understand how all of these independent distributions interact in order to predict our total energy to produce that that polypropylene cup. So let's look at this at sort of the use of Monte Carlo in practice. So this was a study that we had done looking at repair activities performed on a bridge in Norway, the Gimsostrammen Bridge from 1993 to 1995. There were two types of repairs that were undertaken. One is a mechanical repair and the other is just a surface treatment. Mechanical repair is basically jackhammering out the part of the bridge on the side of the girder and removing the bad concrete and replacing it using shotcrete with some new concrete to sort of repair the surface. And then the surface treatment is simply just brushing a hydrophobic agent on the surface and providing some relatively low cost repairs. So these trial repairs were performed in 1983, 94, 95, and they comprise the repair of all the columns and the superstructure of the bridge between piers one and three. So what we're looking at here is a side view of the bridge. And so you can see the cars will be driving up here. And here is the water. And then you can see side view of the bridge right here. It's going on a grade down. And then we have columns supporting the bridge. And we're replacing some sections here and here that are on the side of these pre-stress girders that are, that are, have undergone corrosion and so they need to be replaced. And so here are some maps of, of what each one of those, each one of those components look like, looks like on both the north and the south side of these, of these girders. So for the mechanical repair, you can see it's made up of a hydro demolition. We got to remove the concrete that's there. We're going to shock-crete. We're going to apply a hydrophobic membrane. We're going to do some sandblasting. And then finally a surface treatment. In this case, that's the, that's the full mechanical repair. For the surface treatment, we're only going to do a little bit of water hydro demolition to roughen up the surface and then brush on that surface treatment. Now you can see for each one of these, we have a list of materials, how the, how it's going to actually be transported to the site, what the supply chain looks like, how much energy and fuel is necessary to do it, how many people are required. And then also the cost in this case in Norwegian kroner. Same for the surface treatment. These are the kinds of inputs you get talking to any project sponsor. And so that you just simply ask them, can you, can you give us some insight into what it's going to take to do this and what is the highest you've seen? What is the lowest you've seen? And, and, and, and, and, and, you know, so how long might it take you to, to do this project? How, how much you have to, how much may you have to reduce the, the, the traffic, the traffic capacity of the bridge? How long will you shut the lanes down? Things of that nature. So then you, you break down each one of the, each one of the tasks into specific independent variables that are going to come in. And so here we're looking at the impact in this case of the water hydro demolition. And the impact from the water hydro demolition is going to be the impact to produce the, the, the, the potable water times the rate at which you're going to be using it times the depth of the hydro demolition times the area that you're going to be doing the hydro demolition. So that tells you the impact to produce the water to carry out the hydro demolition. Then there's an impact from the water production for the wash down, which is the impact to, to create the potable water. And then the amount of, of water you're going to be using to wash this thing down afterwards. The impact from landfilling, the hydro demolition concrete is the volume times the density of concrete times the impact of, of waste per kilogram. And then finally impact from operating the equipment itself is going to be the rate at which you're conducting the hydro demolition. And, and, and then multiplying that by the energy consumed or the impacts associated with operating a compressor and then the hydro machine itself, and also a loader, which will, which will be taking away all of the, all the hydro demolition concrete. For a lot of those impact variables, we're looking at, at, at normal distributions. We're looking at uniform distributions for some of these. They're quite simply, they're quite simply deterministic. You know, things like things like the area deterministic. And if, and if we didn't know, sometimes they were deterministic. Sometimes they, they, it was, it was an assumed distribution. And, and then of course you do that for every single one of these. In this case for the, for the shotcrete impact from producing the shotcrete or the concrete that can be sprayed impact from landfilling, the rebound waste that gets rebounded off when you spray the shotcrete on the impact from the shotcrete processing equipment. And then from the material transportation. And you can see all of these are independent variables that get added up. For the surface repair, we only have to deal with the impact due to the hydro demolition and the surface treatment. All of the shotcrete impact variables. And then we run our Monte Carlo analysis. When we do that, ultimately we see this distribution in terms of the kilograms of CO2 equivalents per, in this case, our functional unit as a square meter of repair that's completed. And this is for, in this case, the, the, the mechanical repair. And so what you can see is that it forms a pretty Gaussian distribution. With a mean of 82.1 and a standard deviation, standard deviation of 12.1. Kilograms of CO2 equivalents per square meter of repair. Now, in some regards, it makes sense that this is a normal distribution. And the reason is, if we go back, we're adding up a lot of different distributions. Yes. In this case, because we're multiplying things that may be Gaussian in nature, we would see a log normal. However, because we're adding up a number of distributions, for those that are familiar with it, the central limit theorem absolutely applies. And so as we continue to add distributions, if there are sequential stages to a process, we would expect that our final impacts would normally be Gaussian in nature. So that's how we can then quantify, in terms of greenhouse gases, the impact per square meter of repair. We can do the same thing for another impact category, like acidification potential. And once again, seeing something very Gaussian in nature for all of those Monte Carlo runs. And in this case, we had a mean of 0.635 kilograms of SO2 equivalents per square meter of repair, with a standard deviation of 0.078 kilograms of SO2 equivalents. We then found that all the impact categories were normally distributed. So in this case, for trial repair one, the impacts per square meter ended up being for a global warming potential, ozone depletion, acidification, all of these with their units. Here were the means, the standard deviations, and the coefficients of variation. Similarly, for the surface repair treatment, you can see that, in fact, they're all lower, as we would expect them to be. You're doing less work. However, of course, it doesn't last as long. All right. So the mechanics of Monte Carlo are such that you're going to want to do this in Excel. So, and some useful Excel functions are the rand function, where it's just equals rand, open parentheses, close parentheses. This returns a continuous random variable between 0 and 1 that is uniformly distributed. And so therefore, it's probability distribution looks something like this, as far as a PDF. Equals norm-inv, open parentheses with the probability, the mean, and then the standard deviation, returns the inverse of the normal cumulative distribution for the specified mean and standard deviation. So if you want to pull from a Gaussian distribution, provide it a random variable between 0 and 1, and then generate a number of Monte Carlo runs based on that. Providing the mean and standard deviation, and you will, the underlying distribution will provide the, will provide numbers that fit that distribution. And finally, equals log inverse, open, probability, the log normal of the mean, the log normal of the standard deviation. This returns the inverse of the log normal cumulative distribution for the specified mean and standard deviation. And so these are some useful Excel functions. You'll also find on the course website, a write up on how to generate a triangular distribution in Excel as well. And there's no specific baked in function for that, much like there's not one for a uniform distribution. But, but, but, but just with a few different simple calculations in Excel, you can, you can carry out this kind of Monte Carlo analysis. Now, you don't have to use Excel if you don't want to. There are lots of software tools out there that can help with this. Those, those include Crystal Ball, At Risk, ProbabilityManagement.org for those that have taken Sam Savage's course using SIP Math in Excel. You can use any of those tools to, to, to do your Monte Carlo analysis. Now, ultimately, you're going to want to determine what kind of distribution you have for your, for your, your final answer, or your final distribution for your output. So, say for that cup, was it in fact a normal distribution? Or going back, were these normal distributions as well? This mean the standard deviation? Yes, one of the ways is simply look at the distribution and see if the impacts are normally distributed, but that's not always the best way to go about doing it. So, one of the ways to end up doing this is to try to understand the shape and degree of fit for those distributions. And one way to do this, to test for normality, is to graph the data on what we will call probability paper or on the, on the, on the standard, on the standard deviation axes. So, in order to do this, we'll order the data values in increasing order from our distribution. We're going to determine the cumulative probability for each data value. And so, in order to do that, since we have them all in, in increasing order, that, that cumulative probability will just simply be the, the sort of cumulative of where it is in the overall order divided by the total number of values we have in the distribution. Then we compute the inverse cumulative normal probability for each data value. Basically, this is the standard normal variant. And we can just compute that by using the Excel function equals norm S, I, and V based on that cumulative probability for each data value. We'll then plot the data value on the x-axis versus its standard normal variant on the y-axis. And the best fit linear function and determine the r-squared. If it's close to one, you've got normality. So, in this case, we're looking at a graph on probability paper to say, oh, okay, is, are, are these, is something like maximum annual wind speed, um, a, in normally distributed. And what we see here is probably something that looks like that. Now, I think most of us would say that's probably not linear. And, and, and so therefore this is not a normal distribution. Maximum annual wind speed is not. And actually it shouldn't be, because maximum annual wind speed is an extreme value problem. And so therefore we wouldn't expect it necessarily to be a normal distribution. So if I were to try to fit a line to this, the r-squared would be reasonably low. However, if we look at something like, uh, what's coming off of, um, population estimates, in this case, here is our data. And, right, when I try to fit this line through it, it actually looks quite good. We're going to get an r-squared value that's reasonably high. Right. So in that case, we would say, yes, probably the underlying distribution associated with this is going to be, uh, is going to be normal. All right. And so then finally, how do we report uncertainty associated with this Monte Carlo analysis? Well, often we'll do it with something like a 95% confidence interval. Right. And so that's what you're seeing here for any one of these impacts. A 95% confidence interval. And so once we know if we're normal or log normal, then we can determine what the confidence interval is based on the number of standard deviations that we are from the mean. And so those kinds of error bars then become useful when we're trying to compare between different, different, uh, different results to say, look, you know, is there actually any significant difference between the climate change impacts associated with this product system? What we're looking at here and maybe another, uh, another, uh, another, um, uh, system in which, you know, the, the, the, the, um, with 95% confidence, we can't say the two are, are, are significantly different. And so that's ultimately what we're going to end up using, uh, uh, Monte Carlo analysis, um, uh, to report. So that sort of wraps up our discussion of, uh, of uncertainty in, in, in, in your LCA studies at the very least, uh, you're going to want to do some kind of, um, sensitivity analysis and create a tornado diagram. Um, uh, it, it will probably be a bit difficult to go all the way to a, like a full Monte Carlo or, or some other more sophisticated, uh, uncertainty propagation, uh, tool. But, but at the very least, you're going to want to carry out a sensitivity analysis and provide a, um, a tornado diagram, uh, and some analysis of that tornado diagram as the interpretation part, or as part of the interpretation associated, uh, with, with, with the ISO 14-040 series, uh, standards. So that wraps up uncertainty, uh, and, um, and it's a very important and critical part, uh, to, uh, to, to every, to every LCA.